# -*- coding: utf-8 -*-
"""topic modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VkFd-pFwAYwO_QPMz2geJdZK5oFZCmko
"""

pip install bs4 #package for scraping

from bs4 import BeautifulSoup as bs
import requests #sending request to access the content
from csv import writer # wirting it in csv format
import pandas as pd

websites = ['https://www.deccanherald.com/business/business-news/indian-crude-oil-exports-grow-as-eu-eliminates-russia-1199791.html',
            'https://www.eia.gov/energyexplained/oil-and-petroleum-products/',
            'https://blogs.worldbank.org/developmenttalk/what-triggered-oil-price-plunge-2014-2016-and-why-it-failed-deliver-economic-impetus-eight-charts']
headers_={'user-agent':'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)'}

with open('Topicmodelling_final.csv','w',encoding='utf8',newline='') as f:
  thewriter=writer(f)   
  header=['articles'] 
  thewriter.writerow(header)  
  for website in websites:
    response = requests.get(website)
    html = response.content
    soup = bs(html,'html.parser')
    paragraphs = soup.find_all('p')
    for p in paragraphs:
      para = p.text
      info=[para]
      thewriter.writerow(info)

df=pd.read_csv('Topicmodelling_final.csv')

df['lower1']=df['articles'].apply(lambda x: x.lower())

import string
def remove_punctuation(text):
    punc="".join([i for i in text if i not in string.punctuation])
    return punc
df['punc1']= df['lower1'].apply(lambda x:remove_punctuation(x))

#token
from nltk.tokenize import TweetTokenizer as tt
tokenizer = tt()      # instantiate the tokenizer class
df['token1'] = df['punc1'].apply(lambda x: tokenizer.tokenize(x))

#remove stopwords
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stopword=stopwords.words('english')
def remove_stopwords(text):
    output= [i for i in text if i not in stopword]
    return output
df['stop1']= df['token1'].apply(lambda x:remove_stopwords(x))

from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
#steming
stemmer = PorterStemmer()
def stemming(text):
    stem_text = [stemmer.stem(word) for word in text]
    return stem_text
df['stem1']=df['stop1'].apply(lambda x: stemming(x))


#lemma
nltk.download('wordnet')
nltk.download('omw-1.4')
lemma = WordNetLemmatizer()
def lemmatizer(text):
    lemm_text = [lemma.lemmatize(word) for word in text]
    return lemm_text
df['lemma1']=df['stem1'].apply(lambda x:lemmatizer(x))

df=df.drop(0,axis=0)
df['text']=df['lemma1'].apply(lambda x:' '.join(x))
df

df.to_json("topic_modelling.json")

!pip install pyLDAvis

import numpy as np
import json
import glob

#Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

#spacy
import spacy
import nltk
from nltk.corpus import stopwords

#Vis
import pyLDAvis
import pyLDAvis.gensim

import warnings 
warnings.filterwarnings("ignore",category=DeprecationWarning)

def load_data(file):
  with open(file,"r",encoding="utf-8") as f:
    data=json.load(f)
  return(data)

def write_data(file,data):
  with open(file,"w",encoding="utf-8") as f:
    json.dump(data,f,indent=4)

data=load_data("/content/topic_modelling.json")["articles"]
data=data["80"]
data=data.split(".")
data

def lemmatization(texts,allowed_postages=["NOUN","ADJ","VERB","ADV"]):
  nlp=spacy.load("en_core_web_sm",disable=["senter","ner","parser"])
  texts_out=[]
  for text in texts:
    doc=nlp(text)
    new_text=[]
    for token in doc:
      if token.pos_ in allowed_postages:
        new_text.append(token.lemma_)
    final=" ".join(new_text)
    texts_out.append(final)
  return (texts_out)

lemmatized_texts=lemmatization(data)
lemmatized_texts

def gen_words(texts):
  final=[]
  for text in texts:
    new=gensim.utils.simple_preprocess(text,deacc=True)
    final.append(new)
  return(final)

data_words=gen_words(lemmatized_texts)
print(data_words)

id2word=corpora.Dictionary(data_words)
corpus=[]
for text in data_words:
  new = id2word.doc2bow(text)
  corpus.append(new)

print(corpus)

word = id2word
print(word)

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=10,
                                           passes=10,
                                           alpha='auto',
                                           eta='auto')

pyLDAvis.enable_notebook()
vis=pyLDAvis.gensim.prepare(lda_model,corpus,id2word,mds="mmds",R=30)
vis

























